{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from convert_to_record import read_and_decode_image\n",
    "\n",
    "def train_data(images_tfrecord, masks_tfrecord, size=(580, 420)):\n",
    "    images_queue = tf.train.string_input_producer([images_tfrecord])\n",
    "    image = read_and_decode_image(images_queue, size)\n",
    "    \n",
    "    masks_queue = tf.train.string_input_producer([masks_tfrecord])\n",
    "    mask = tf.to_float(tf.greater(read_and_decode_image(masks_queue, size)[:,:, 0], 0))\n",
    "    return image, mask\n",
    "\n",
    "def preprocess(image):\n",
    "    # First, the image is essentially grayscale.\n",
    "    gray_image = tf.image.rgb_to_grayscale(image)\n",
    "    \n",
    "    # Then we want to normalize.\n",
    "    normalized_image = (1.0/255)*(tf.to_float(gray_image) - 127.0)\n",
    "    \n",
    "    # We'll likely also want to produce random perturbation of the\n",
    "    # data to synthetically increase the size of the data set\n",
    "    # TODO(kjchavez): Add random jitter.\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def batched_train_data(images_tfrecord, masks_tfrecord, batch_size=32, size=(580, 420)):\n",
    "    image, mask = train_data(images_tfrecord, masks_tfrecord, size=size)\n",
    "    image = preprocess(image)\n",
    "    image_batch, mask_batch = tf.train.batch([image, mask], batch_size)\n",
    "    return image_batch, mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictive model.\n",
    "\n",
    "def predict(X):\n",
    "    \"\"\" Produces logits for each pixel in a training image, corresponding to\n",
    "        the probability that it is part of the sought-after nerve.\n",
    "    \"\"\"\n",
    "    # Allocate variables on GPU.\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        W1 = tf.get_variable(\"w1\", [5, 5, 1, 32], initializer=tf.truncated_normal_initializer(stddev=0.2))\n",
    "        b1 = tf.get_variable(\"b1\", [32], initializer=tf.constant_initializer(0.0))\n",
    "        W2 = tf.get_variable(\"w2\", [1, 1, 32, 1], initializer=tf.truncated_normal_initializer(stddev=0.2))\n",
    "        b2 = tf.get_variable(\"b2\", [1], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "    conv1 = tf.nn.conv2d(X, W1, strides=(1,1,1,1), padding='SAME', name=\"conv\")\n",
    "    a1 = tf.nn.relu(tf.nn.bias_add(conv1, b1))\n",
    "    conv2 = tf.nn.conv2d(a1, W2, strides=(1,1,1,1), padding='SAME', name=\"net_in_net\")\n",
    "    return tf.squeeze(tf.nn.bias_add(conv2, b2))\n",
    "\n",
    "def prob(logits):\n",
    "    output_prob = tf.nn.sigmoid(logits)\n",
    "    return output_prob\n",
    "\n",
    "def xentropy_loss(logits, y):\n",
    "    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits, y, name=\"xentropy_loss\")\n",
    "    return tf.reduce_mean(xentropy)\n",
    "\n",
    "def predict_mask(probs, threshold=0.5):\n",
    "    bool_mask = probs > threshold\n",
    "    return bool_mask\n",
    "\n",
    "def dice_coefficient(pred, label):\n",
    "    pred = tf.to_int32(pred)\n",
    "    label = tf.to_int32(label)\n",
    "    positive_in_pred = tf.reduce_sum(pred, reduction_indices=[1, 2])\n",
    "    positive_in_label = tf.reduce_sum(label, reduction_indices=[1, 2])\n",
    "    denom = tf.to_float(positive_in_pred + positive_in_label)\n",
    "    numer = 2.0 * tf.to_float(tf.reduce_sum(pred * label, reduction_indices=[1,2]))\n",
    "\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from train import *\n",
    "\n",
    "# Build a model\n",
    "X, y = batched_train_data(\"train_images.tfrecord\", \"train_masks.tfrecord\")\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=False) as scope:\n",
    "    logits = predict(X)\n",
    "    y_pred = prob(logits)\n",
    "\n",
    "loss = xentropy_loss(logits, y)\n",
    "score = dice_coefficient(predict_mask(y_pred, threshold=0.5), y)\n",
    "avg_score = tf.reduce_mean(score)\n",
    "\n",
    "lr = 1e-3\n",
    "train_op = get_sgd_train_op(loss, lr)\n",
    "\n",
    "def evaluate():\n",
    "    sess = tf.get_default_session()\n",
    "    avg_score_value = sess.run(avg_score)\n",
    "    return \"Average Dice Coefficient on training minibatch: %0.4f\" % avg_score_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ iteration 00000: 0.777571\n",
      "Loss @ iteration 00010: 0.777847\n",
      "Loss @ iteration 00020: 0.777602\n",
      "Loss @ iteration 00030: 0.762776\n",
      "Loss @ iteration 00040: 0.763489\n",
      "Loss @ iteration 00050: 0.758508\n",
      "Loss @ iteration 00060: 0.748659\n",
      "Loss @ iteration 00070: 0.748529\n",
      "Loss @ iteration 00080: 0.731098\n",
      "Loss @ iteration 00090: 0.729687\n",
      "Loss @ iteration 00100: 0.715489\n",
      "Average Dice Coefficient on training minibatch: 0.0125\n",
      "Loss @ iteration 00110: 0.714883\n",
      "Loss @ iteration 00120: 0.705453\n",
      "Loss @ iteration 00130: 0.702829\n",
      "Loss @ iteration 00140: 0.699686\n",
      "Loss @ iteration 00150: 0.692023\n",
      "Loss @ iteration 00160: 0.681758\n",
      "Loss @ iteration 00170: 0.684437\n",
      "Loss @ iteration 00180: 0.678026\n",
      "Loss @ iteration 00190: 0.669673\n",
      "Loss @ iteration 00200: 0.668086\n",
      "Average Dice Coefficient on training minibatch: 0.0089\n",
      "Loss @ iteration 00210: 0.658942\n",
      "Loss @ iteration 00220: 0.656159\n",
      "Loss @ iteration 00230: 0.647740\n",
      "Loss @ iteration 00240: 0.643756\n",
      "Loss @ iteration 00250: 0.636938\n",
      "Loss @ iteration 00260: 0.631776\n",
      "Loss @ iteration 00270: 0.627612\n",
      "Loss @ iteration 00280: 0.621947\n",
      "Loss @ iteration 00290: 0.621414\n",
      "Loss @ iteration 00300: 0.615616\n",
      "Average Dice Coefficient on training minibatch: 0.0002\n",
      "Loss @ iteration 00310: 0.609565\n",
      "Loss @ iteration 00320: 0.608839\n",
      "Loss @ iteration 00330: 0.603476\n",
      "Loss @ iteration 00340: 0.597674\n",
      "Loss @ iteration 00350: 0.595158\n",
      "Loss @ iteration 00360: 0.589508\n",
      "Loss @ iteration 00370: 0.586250\n",
      "Loss @ iteration 00380: 0.579639\n",
      "Loss @ iteration 00390: 0.576161\n",
      "Loss @ iteration 00400: 0.568228\n",
      "Average Dice Coefficient on training minibatch: nan\n",
      "Loss @ iteration 00410: 0.564626\n",
      "Loss @ iteration 00420: 0.561632\n",
      "Loss @ iteration 00430: 0.558719\n",
      "Loss @ iteration 00440: 0.555868\n",
      "Loss @ iteration 00450: 0.551108\n",
      "Loss @ iteration 00460: 0.551006\n",
      "Loss @ iteration 00470: 0.549424\n",
      "Loss @ iteration 00480: 0.544536\n",
      "Loss @ iteration 00490: 0.539279\n",
      "Loss @ iteration 00500: 0.532453\n",
      "Average Dice Coefficient on training minibatch: nan\n",
      "Loss @ iteration 00510: 0.528439\n",
      "Loss @ iteration 00520: 0.522961\n",
      "Loss @ iteration 00530: 0.518368\n",
      "Loss @ iteration 00540: 0.518897\n",
      "Loss @ iteration 00550: 0.514781\n",
      "Loss @ iteration 00560: 0.512413\n",
      "Loss @ iteration 00570: 0.509034\n",
      "Loss @ iteration 00580: 0.504863\n",
      "Loss @ iteration 00590: 0.504461\n",
      "Loss @ iteration 00600: 0.498733\n",
      "Average Dice Coefficient on training minibatch: nan\n",
      "Loss @ iteration 00610: 0.496620\n",
      "Loss @ iteration 00620: 0.493804\n",
      "Loss @ iteration 00630: 0.490290\n",
      "Loss @ iteration 00640: 0.486336\n",
      "Loss @ iteration 00650: 0.482594\n",
      "Loss @ iteration 00660: 0.477871\n",
      "Loss @ iteration 00670: 0.477656\n",
      "Loss @ iteration 00680: 0.474301\n",
      "Loss @ iteration 00690: 0.471124\n",
      "Loss @ iteration 00700: 0.467330\n",
      "Average Dice Coefficient on training minibatch: nan\n",
      "Loss @ iteration 00710: 0.464522\n",
      "Loss @ iteration 00720: 0.464951\n",
      "Loss @ iteration 00730: 0.462137\n",
      "Loss @ iteration 00740: 0.455918\n",
      "Loss @ iteration 00750: 0.456470\n",
      "Loss @ iteration 00760: 0.451402\n",
      "Loss @ iteration 00770: 0.448964\n",
      "Loss @ iteration 00780: 0.447912\n",
      "Loss @ iteration 00790: 0.443978\n",
      "Loss @ iteration 00800: 0.442939\n",
      "Average Dice Coefficient on training minibatch: nan\n",
      "Loss @ iteration 00810: 0.441967\n",
      "Loss @ iteration 00820: 0.435872\n"
     ]
    }
   ],
   "source": [
    "train(train_op, evaluate, loss, 1000,\n",
    "      print_every_n=10,\n",
    "      eval_every_n=100,\n",
    "      snapshot_every_n=100,\n",
    "      summarize_every_n=10,\n",
    "      diverge_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the model with raw TensorFlow, no helper function.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "num_batches = 10\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # Do stuff\n",
    "    print \"Starting training.\"\n",
    "    for n in xrange(num_batches):\n",
    "        _, y_val, y_pred_val, loss_value, avg_score_value = sess.run([train_op, y, y_pred, loss, score])\n",
    "        print loss_value\n",
    "        print avg_score_value\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0a2a0b432ae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sess = tf.get_default_session()\n",
    "y_val, y_pred_val = sess.run([y_val, y_pred_val])\n",
    "\n",
    "idx = 1\n",
    "#t = np.reshape(y_val[idx], (580, 420))\n",
    "#t_hat = np.reshape(y_pred_val[idx], (580, 420)) > 0.5\n",
    "t = y_val[idx]\n",
    "t_hat = y_pred_val[idx] > 0.5\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(t, cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(t_hat.astype(np.float), cmap='gray', clim=(0, 1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
